{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./dataset/org/H1_nav.csv')\n",
    "df.head()\n",
    "drop = ['Participant', 'Dataset', 'ViewFrame']\n",
    "df.drop(drop, inplace =True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.shape\n",
    "df = torch.Tensor(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = df[:,0]\n",
    "X = df[:,1]\n",
    "Y = df[:,2]\n",
    "Z = df[:,3]\n",
    "Pitch = df[:,4]\n",
    "Roll = df[:,5]\n",
    "Yaw = df[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(d, p):\n",
    "    data = []\n",
    "    for i in range(0,int(np.ma.count(d)/p)):\n",
    "        if(np.ma.count(d) < i*p):\n",
    "            n = (d[-1] - d[(i*p)])/(np.ma.count(d) - (i*p))\n",
    "            data.append(n)\n",
    "        else:\n",
    "            n = (d[(i*p) + p - 1] - d[(i*p)])/p\n",
    "            data.append(n)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, z, yaw, pitch, roll, n):\n",
    "    x1 = clean(x,n)\n",
    "    y1 = clean(y,n)\n",
    "    z1 = clean(z,n)\n",
    "    yaw1 = clean(yaw, n)\n",
    "    pitch1 = clean(pitch, n)\n",
    "    roll1 = clean(roll, n)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(x1, label = 'X')\n",
    "    plt.plot(y1, label = 'Y')\n",
    "    plt.plot(z1, label = 'Z')\n",
    "    plt.title(\"Body Motion Speed\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"BodyMotion.jpg\", dpi=200)\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(yaw1, label = 'yaw')\n",
    "    plt.plot(pitch1, label = 'pitch')\n",
    "    plt.plot(roll1, label = 'roll')\n",
    "    plt.title(\"Head Motion Speed\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"HeadMotion.jpg\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(data, step=1):\n",
    "    x_d, y_d = [], []\n",
    "    for i in range(len(data)-step-1):\n",
    "        a = data[i:(i+step), 0]\n",
    "        x_d.append(a)\n",
    "        y_d.append(data[i + step, 0])\n",
    "    return np.array(x_d), np.array(y_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(n, step = 125):\n",
    "    set=n.reshape(-1, 1)\n",
    "    train_size = int(len(set) * 0.9)\n",
    "    test_size = len(set) - train_size\n",
    "    train, test = set[0:train_size,:], set[train_size:len(set),:]\n",
    "\n",
    "    x_train, y_train = data(train, step)\n",
    "    x_test, y_test = data(test, step)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    x_train = torch.from_numpy(x_train).float()\n",
    "    y_train = torch.from_numpy(y_train).float()\n",
    "    x_test = torch.from_numpy(x_test).float()\n",
    "    y_test = torch.from_numpy(y_test).float()\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self, hidden_layers=32, numLayers = 1):\n",
    "        super(lstm, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.numLayers = numLayers\n",
    "        self.lstm1 = nn.LSTM(125, self.hidden_layers, batch_first=True, num_layers = numLayers)\n",
    "        self.linear = nn.Linear(self.hidden_layers, 1)\n",
    "        \n",
    "    def forward(self, inputVal):\n",
    "        hidden_state = torch.randn(self.numLayers, inputVal.size(0), 32).float()\n",
    "        cell_state = torch.randn(self.numLayers, inputVal.size(0), 32).float()\n",
    "        hidden = (hidden_state, cell_state)\n",
    "        h_t, c_t = self.lstm1(inputVal.float(), hidden)\n",
    "        outputs = self.linear(h_t) \n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, model, optimiser, loss_fn, train_input, train_target):\n",
    "    model.train()\n",
    "    trainLoss = []\n",
    "    train_target = train_target.reshape(train_target.size(0), 1, 1)\n",
    "    for i in range(n_epochs):\n",
    "        optimiser.zero_grad()\n",
    "        out = model(train_input)\n",
    "        loss = loss_fn(out, train_target)\n",
    "        loss.backward()    \n",
    "        optimiser.step()\n",
    "        trainLoss.append(loss)\n",
    "        print(\"Step: {}, Loss: {}\".format(i, loss))\n",
    "    return trainLoss\n",
    "\n",
    "def eval(model, loss_fn, test_input, test_target):\n",
    "        model.eval()\n",
    "        test_target = test_target.reshape(test_target.size(0), 1, 1)\n",
    "        with torch.no_grad():\n",
    "            pred = model(test_input)\n",
    "            loss = loss_fn(pred, test_target)\n",
    "            return loss, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a(epoch, model, optimiser, criterion, x_train, y_train, path, n):\n",
    "    model = model.float()\n",
    "    train_loss = train(epoch, model, optimiser, criterion, x_train, y_train)\n",
    "    l = []\n",
    "    for i in range(0,len(train_loss)):\n",
    "        l.append(train_loss[i].item())\n",
    "    plt.plot(l, label = \"Training Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(path+n+\"train_loss.jpg\", dpi=200)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_a(model, criterion, x_test, y_test, path, n):\n",
    "    test_loss, prediction = eval(model, criterion, x_test, y_test)\n",
    "    print(test_loss.item())\n",
    "    plt.figure(figsize=(8, 6), dpi=100)\n",
    "    plt.plot(y_test, prediction.reshape(prediction.size(0)))\n",
    "    plt.xlabel(\"Actual Value\")\n",
    "    plt.ylabel(\"Predicted Value\")\n",
    "    plt.savefig(path+n+\"p.jpg\", dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for HMDPX\n",
      "Step: 0, Loss: 0.23395048081874847\n",
      "Step: 1, Loss: 0.22194938361644745\n",
      "Step: 2, Loss: 0.21345995366573334\n",
      "Step: 3, Loss: 0.210422083735466\n",
      "Step: 4, Loss: 0.20766006410121918\n",
      "Step: 5, Loss: 0.20397403836250305\n",
      "Step: 6, Loss: 0.2036508470773697\n",
      "Step: 7, Loss: 0.20072832703590393\n",
      "Step: 8, Loss: 0.20111265778541565\n",
      "Step: 9, Loss: 0.20023629069328308\n",
      "Step: 10, Loss: 0.2001207321882248\n",
      "Step: 11, Loss: 0.19915437698364258\n",
      "Step: 12, Loss: 0.19903457164764404\n",
      "Step: 13, Loss: 0.19905893504619598\n",
      "Step: 14, Loss: 0.19855256378650665\n",
      "Step: 15, Loss: 0.198441281914711\n",
      "Step: 16, Loss: 0.19794607162475586\n",
      "Step: 17, Loss: 0.1980246752500534\n",
      "Step: 18, Loss: 0.19781680405139923\n",
      "Step: 19, Loss: 0.19746823608875275\n",
      "Step: 20, Loss: 0.1976865530014038\n",
      "Step: 21, Loss: 0.1979721188545227\n",
      "Step: 22, Loss: 0.1979227215051651\n",
      "Step: 23, Loss: 0.19784508645534515\n",
      "Step: 24, Loss: 0.1979789435863495\n",
      "Step: 25, Loss: 0.19769909977912903\n",
      "Step: 26, Loss: 0.19779327511787415\n",
      "Step: 27, Loss: 0.19784653186798096\n",
      "Step: 28, Loss: 0.19781513512134552\n",
      "Step: 29, Loss: 0.19779139757156372\n",
      "Step: 30, Loss: 0.1979130059480667\n",
      "Step: 31, Loss: 0.197632297873497\n",
      "Step: 32, Loss: 0.19766469299793243\n",
      "Step: 33, Loss: 0.19775021076202393\n",
      "Step: 34, Loss: 0.19780609011650085\n",
      "Step: 35, Loss: 0.1977604478597641\n",
      "Step: 36, Loss: 0.19791029393672943\n",
      "Step: 37, Loss: 0.1978333741426468\n",
      "Step: 38, Loss: 0.1974978893995285\n",
      "Step: 39, Loss: 0.19761964678764343\n",
      "Step: 40, Loss: 0.19770586490631104\n",
      "Step: 41, Loss: 0.1977829933166504\n",
      "Step: 42, Loss: 0.19776880741119385\n",
      "Step: 43, Loss: 0.197631374001503\n",
      "Step: 44, Loss: 0.19764664769172668\n",
      "Step: 45, Loss: 0.19759926199913025\n",
      "Step: 46, Loss: 0.19771140813827515\n",
      "Step: 47, Loss: 0.197677880525589\n",
      "Step: 48, Loss: 0.19773122668266296\n",
      "Step: 49, Loss: 0.19760668277740479\n",
      "Step: 50, Loss: 0.19764883816242218\n",
      "Step: 51, Loss: 0.1977524310350418\n",
      "Step: 52, Loss: 0.1976473480463028\n",
      "Step: 53, Loss: 0.19766898453235626\n",
      "Step: 54, Loss: 0.19759279489517212\n",
      "Step: 55, Loss: 0.1976926326751709\n",
      "Step: 56, Loss: 0.1976419985294342\n",
      "Step: 57, Loss: 0.19769443571567535\n",
      "Step: 58, Loss: 0.19770634174346924\n",
      "Step: 59, Loss: 0.19773977994918823\n",
      "Step: 60, Loss: 0.19756926596164703\n",
      "Step: 61, Loss: 0.19767090678215027\n",
      "Step: 62, Loss: 0.19761797785758972\n",
      "Step: 63, Loss: 0.19762365520000458\n",
      "Step: 64, Loss: 0.19770118594169617\n",
      "Step: 65, Loss: 0.19767068326473236\n",
      "Step: 66, Loss: 0.19764916598796844\n",
      "Step: 67, Loss: 0.19767440855503082\n",
      "Step: 68, Loss: 0.19758324325084686\n",
      "Step: 69, Loss: 0.19768126308918\n",
      "Step: 70, Loss: 0.19772101938724518\n",
      "Step: 71, Loss: 0.1975737363100052\n",
      "Step: 72, Loss: 0.1976306140422821\n",
      "Step: 73, Loss: 0.1976550817489624\n",
      "Step: 74, Loss: 0.19758936762809753\n",
      "Step: 75, Loss: 0.197623610496521\n",
      "Step: 76, Loss: 0.19763733446598053\n",
      "Step: 77, Loss: 0.197632297873497\n",
      "Step: 78, Loss: 0.19764471054077148\n",
      "Step: 79, Loss: 0.1976357102394104\n",
      "Step: 80, Loss: 0.19762861728668213\n",
      "Step: 81, Loss: 0.19764195382595062\n",
      "Step: 82, Loss: 0.19762523472309113\n",
      "Step: 83, Loss: 0.19765500724315643\n",
      "Step: 84, Loss: 0.19764210283756256\n",
      "Step: 85, Loss: 0.1976606249809265\n",
      "Step: 86, Loss: 0.1975720226764679\n",
      "Step: 87, Loss: 0.19761984050273895\n",
      "Step: 88, Loss: 0.1976064294576645\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/arty/Desktop/NYU Spring 2022/6DOF/predicting_stepbystep.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000012?line=12'>13</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000012?line=13'>14</a>\u001b[0m optimiser \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000012?line=14'>15</a>\u001b[0m train_a(\u001b[39m100\u001b[39;49m, model, optimiser, criterion, t_x, t_y,file,i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000012?line=15'>16</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), path\u001b[39m+\u001b[39mi\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msavedModel.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000012?line=16'>17</a>\u001b[0m eval_a(model, criterion, e_x, e_y, file, i)\n",
      "\u001b[1;32m/home/arty/Desktop/NYU Spring 2022/6DOF/predicting_stepbystep.ipynb Cell 11'\u001b[0m in \u001b[0;36mtrain_a\u001b[0;34m(epoch, model, optimiser, criterion, x_train, y_train, path, n)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_a\u001b[39m(epoch, model, optimiser, criterion, x_train, y_train, path, n):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000010?line=1'>2</a>\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000010?line=2'>3</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(epoch, model, optimiser, criterion, x_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000010?line=3'>4</a>\u001b[0m     l \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000010?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(train_loss)):\n",
      "\u001b[1;32m/home/arty/Desktop/NYU Spring 2022/6DOF/predicting_stepbystep.ipynb Cell 10'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, model, optimiser, loss_fn, train_input, train_target)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000009?line=6'>7</a>\u001b[0m out \u001b[39m=\u001b[39m model(train_input)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000009?line=7'>8</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(out, train_target)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000009?line=8'>9</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()    \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000009?line=9'>10</a>\u001b[0m optimiser\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arty/Desktop/NYU%20Spring%202022/6DOF/predicting_stepbystep.ipynb#ch0000009?line=10'>11</a>\u001b[0m trainLoss\u001b[39m.\u001b[39mappend(loss)\n",
      "File \u001b[0;32m~/anaconda3/envs/6/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/6/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/arty/anaconda3/envs/6/lib/python3.9/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    n = r\"dataset/H\" + str(i) +  \".csv\"\n",
    "    path = r\"dataset/H\" + str(i) + \"_nav/\"\n",
    "    file = r\"dataset/H\" + str(i) + \"_nav/plots/\"\n",
    "    df = pd.read_csv (n)\n",
    "    \n",
    "    for (i, j) in df.iteritems():\n",
    "        print(\"Predicting for \" + i)\n",
    "        c = df[i].to_numpy()\n",
    "        t_x, t_y, e_x, e_y = process(c)\n",
    "        \n",
    "        model = lstm(numLayers = 10)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimiser = optim.Adam(model.parameters(), lr=0.01)\n",
    "        train_a(100, model, optimiser, criterion, t_x, t_y,file,i)\n",
    "        torch.save(model.state_dict(), path+i+\"savedModel.pt\")\n",
    "        eval_a(model, criterion, e_x, e_y, file, i)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71ddd5e9409eaf14d4d4bd6327be54b02490eb403d295c16528195f3b190b56b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('multimodal-priors')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
