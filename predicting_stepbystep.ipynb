{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./dataset/H1_nav.csv')\n",
    "df.head()\n",
    "drop = ['FrameNumber', 'Participant', 'Dataset', 'ViewFrame']\n",
    "df.drop(drop, inplace =True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>HMDRX</th>\n",
       "      <th>HMDRY</th>\n",
       "      <th>HMDRZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0500</td>\n",
       "      <td>1.7868</td>\n",
       "      <td>-1.0947</td>\n",
       "      <td>6.9163</td>\n",
       "      <td>350.8206</td>\n",
       "      <td>359.9912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0498</td>\n",
       "      <td>1.7871</td>\n",
       "      <td>-1.0951</td>\n",
       "      <td>6.9116</td>\n",
       "      <td>351.1272</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0498</td>\n",
       "      <td>1.7872</td>\n",
       "      <td>-1.0955</td>\n",
       "      <td>6.8915</td>\n",
       "      <td>351.3081</td>\n",
       "      <td>0.0221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0498</td>\n",
       "      <td>1.7870</td>\n",
       "      <td>-1.0961</td>\n",
       "      <td>6.9375</td>\n",
       "      <td>351.5385</td>\n",
       "      <td>359.9731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0497</td>\n",
       "      <td>1.7865</td>\n",
       "      <td>-1.0969</td>\n",
       "      <td>7.1456</td>\n",
       "      <td>351.7421</td>\n",
       "      <td>359.8803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x       y       z   HMDRX     HMDRY     HMDRZ\n",
       "0  0.0500  1.7868 -1.0947  6.9163  350.8206  359.9912\n",
       "1  0.0498  1.7871 -1.0951  6.9116  351.1272    0.0094\n",
       "2  0.0498  1.7872 -1.0955  6.8915  351.3081    0.0221\n",
       "3  0.0498  1.7870 -1.0961  6.9375  351.5385  359.9731\n",
       "4  0.0497  1.7865 -1.0969  7.1456  351.7421  359.8803"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timePointList(dataList, timePointBracket):\n",
    "    tpList = []\n",
    "    for i in range(0,int(np.ma.count(dataList)/timePointBracket)):\n",
    "        if(np.ma.count(dataList) < i*timePointBracket):\n",
    "            #This is the last iteration\n",
    "            delta = (dataList[-1] - dataList[(i*timePointBracket)])/(np.ma.count(dataList) - (i*timePointBracket))\n",
    "            tpList.append(delta)\n",
    "        else:\n",
    "            delta = (dataList[(i*timePointBracket) + timePointBracket - 1] - dataList[(i*timePointBracket)])/timePointBracket\n",
    "            tpList.append(delta)\n",
    "    return tpList\n",
    "\n",
    "def generateMotionGraphs(dataX, dataY, dataZ, dataYaw, dataPitch, dataRoll, timepoints):\n",
    "    deltaX = timePointList(dataX,timepoints)\n",
    "    deltaY = timePointList(dataY,timepoints)\n",
    "    deltaZ = timePointList(dataZ,timepoints)\n",
    "    deltaYaw = timePointList(dataYaw, timepoints)\n",
    "    deltaPitch = timePointList(dataPitch, timepoints)\n",
    "    deltaRoll = timePointList(dataRoll, timepoints)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(deltaX, label = '△X')\n",
    "    plt.plot(deltaY, label = '△Y')\n",
    "    plt.plot(deltaZ, label = '△Z')\n",
    "    plt.title(\"Body Motion Speed\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"BodyMotion.jpg\", dpi=200)\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(deltaYaw, label = '△Yaw')\n",
    "    plt.plot(deltaPitch, label = '△Pitch')\n",
    "    plt.plot(deltaRoll, label = '△Roll')\n",
    "    plt.title(\"Head Motion Speed\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"HeadMotion.jpg\", dpi=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset1(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def generateDataset(dataArr, look_back = 125):\n",
    "    dataset=dataArr.reshape(-1, 1)\n",
    "    train_size = int(len(dataset) * 0.9)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "    trainX, trainY = create_dataset1(train, look_back)\n",
    "    testX, testY = create_dataset1(test, look_back)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "    trainX = torch.from_numpy(trainX).float()\n",
    "    trainY = torch.from_numpy(trainY).float()\n",
    "    testX = torch.from_numpy(testX).float()\n",
    "    testY = torch.from_numpy(testY).float()\n",
    "    return trainX, trainY, testX, testY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM2(nn.Module):\n",
    "    def __init__(self, hidden_layers=32, numLayers = 1):\n",
    "        super(LSTM2, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.numLayers = numLayers\n",
    "        self.lstm1 = nn.LSTM(125, self.hidden_layers, batch_first=True, num_layers = numLayers)\n",
    "        self.linear = nn.Linear(self.hidden_layers, 1)\n",
    "        \n",
    "    def forward(self, inputVal):\n",
    "        hidden_state = torch.randn(self.numLayers, inputVal.size(0), 32).float()\n",
    "        cell_state = torch.randn(self.numLayers, inputVal.size(0), 32).float()\n",
    "        hidden = (hidden_state, cell_state)\n",
    "        h_t, c_t = self.lstm1(inputVal.float(), hidden)\n",
    "        outputs = self.linear(h_t) # output from the last FC layer\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, model, optimiser, loss_fn, train_input, train_target):\n",
    "    model.train()\n",
    "    trainLoss = []\n",
    "    train_target = train_target.reshape(train_target.size(0), 1, 1)\n",
    "    for i in range(n_epochs):\n",
    "        optimiser.zero_grad()\n",
    "        out = model(train_input)\n",
    "        loss = loss_fn(out, train_target)\n",
    "        loss.backward()    \n",
    "        optimiser.step()\n",
    "        trainLoss.append(loss)\n",
    "        print(\"Step: {}, Loss: {}\".format(i, loss))\n",
    "    return trainLoss\n",
    "\n",
    "def eval_model(model, loss_fn, test_input, test_target):\n",
    "        model.eval()\n",
    "        test_target = test_target.reshape(test_target.size(0), 1, 1)\n",
    "        with torch.no_grad():\n",
    "            pred = model(test_input)\n",
    "            loss = loss_fn(pred, test_target)\n",
    "            return loss, pred\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(num_epochs, model, optimiser, criterion, trainX, trainY, plotsFolderName, colName):\n",
    "    model = model.float()\n",
    "    train_loss = training_loop(num_epochs, model, optimiser, criterion, trainX, trainY)\n",
    "    lossArr = []\n",
    "    for i in range(0,len(train_loss)):\n",
    "        lossArr.append(train_loss[i].item())\n",
    "    plt.plot(lossArr, label = \"Training Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(plotsFolderName+colName+\"trainLoss.jpg\", dpi=200)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model, criterion, testX, testY, plotsFolderName, colName):\n",
    "    test_loss, prediction = eval_model(model, criterion, testX, testY)\n",
    "    print(test_loss.item())\n",
    "    plt.figure(figsize=(8, 6), dpi=100)\n",
    "    plt.scatter(testY, prediction.reshape(prediction.size(0)))\n",
    "    plt.xlabel(\"Actual Value\")\n",
    "    plt.ylabel(\"Predicted Value\")\n",
    "    plt.savefig(plotsFolderName+colName+\"scatter.jpg\", dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for x\n",
      "Step: 0, Loss: 0.2251625657081604\n",
      "Step: 1, Loss: 0.2168758660554886\n",
      "Step: 2, Loss: 0.21010085940361023\n",
      "Step: 3, Loss: 0.20663990080356598\n",
      "Step: 4, Loss: 0.20437437295913696\n",
      "Step: 5, Loss: 0.203459694981575\n",
      "Step: 6, Loss: 0.20214691758155823\n",
      "Step: 7, Loss: 0.20075364410877228\n",
      "Step: 8, Loss: 0.1997332125902176\n",
      "Step: 9, Loss: 0.19994626939296722\n",
      "Step: 10, Loss: 0.1991107016801834\n",
      "Step: 11, Loss: 0.19890093803405762\n",
      "Step: 12, Loss: 0.1988217532634735\n",
      "Step: 13, Loss: 0.19883635640144348\n",
      "Step: 14, Loss: 0.19873616099357605\n",
      "Step: 15, Loss: 0.19839806854724884\n",
      "Step: 16, Loss: 0.19801650941371918\n",
      "Step: 17, Loss: 0.19789515435695648\n",
      "Step: 18, Loss: 0.19750601053237915\n",
      "Step: 19, Loss: 0.19785916805267334\n",
      "Step: 20, Loss: 0.19775508344173431\n",
      "Step: 21, Loss: 0.19787341356277466\n",
      "Step: 22, Loss: 0.19780349731445312\n",
      "Step: 23, Loss: 0.19755910336971283\n",
      "Step: 24, Loss: 0.19781582057476044\n",
      "Step: 25, Loss: 0.19788585603237152\n",
      "Step: 26, Loss: 0.19814078509807587\n",
      "Step: 27, Loss: 0.1978202611207962\n",
      "Step: 28, Loss: 0.19796936213970184\n",
      "Step: 29, Loss: 0.1978297382593155\n",
      "Step: 30, Loss: 0.19783468544483185\n",
      "Step: 31, Loss: 0.19782665371894836\n",
      "Step: 32, Loss: 0.19792556762695312\n",
      "Step: 33, Loss: 0.19770558178424835\n",
      "Step: 34, Loss: 0.19803957641124725\n",
      "Step: 35, Loss: 0.19774331152439117\n",
      "Step: 36, Loss: 0.1976364403963089\n",
      "Step: 37, Loss: 0.19774992763996124\n",
      "Step: 38, Loss: 0.19772697985172272\n",
      "Step: 39, Loss: 0.1979166567325592\n",
      "Step: 40, Loss: 0.19758747518062592\n",
      "Step: 41, Loss: 0.19765762984752655\n",
      "Step: 42, Loss: 0.1977165788412094\n",
      "Step: 43, Loss: 0.1976175755262375\n",
      "Step: 44, Loss: 0.197628915309906\n",
      "Step: 45, Loss: 0.19781412184238434\n",
      "Step: 46, Loss: 0.1977396458387375\n",
      "Step: 47, Loss: 0.19783498346805573\n",
      "Step: 48, Loss: 0.19779780507087708\n",
      "Step: 49, Loss: 0.19778600335121155\n",
      "Step: 50, Loss: 0.19759967923164368\n",
      "Step: 51, Loss: 0.19771546125411987\n",
      "Step: 52, Loss: 0.1977546662092209\n",
      "Step: 53, Loss: 0.19773252308368683\n",
      "Step: 54, Loss: 0.1977367252111435\n",
      "Step: 55, Loss: 0.1976086050271988\n",
      "Step: 56, Loss: 0.1977061927318573\n",
      "Step: 57, Loss: 0.19766120612621307\n",
      "Step: 58, Loss: 0.19768467545509338\n",
      "Step: 59, Loss: 0.19767633080482483\n",
      "Step: 60, Loss: 0.19753625988960266\n",
      "Step: 61, Loss: 0.19764642417430878\n",
      "Step: 62, Loss: 0.19773416221141815\n",
      "Step: 63, Loss: 0.1977258324623108\n",
      "Step: 64, Loss: 0.19769100844860077\n",
      "Step: 65, Loss: 0.19765935838222504\n",
      "Step: 66, Loss: 0.19765964150428772\n",
      "Step: 67, Loss: 0.1976236253976822\n",
      "Step: 68, Loss: 0.1976555734872818\n",
      "Step: 69, Loss: 0.1976340413093567\n",
      "Step: 70, Loss: 0.1976201981306076\n",
      "Step: 71, Loss: 0.19766218960285187\n",
      "Step: 72, Loss: 0.1977902054786682\n",
      "Step: 73, Loss: 0.19757455587387085\n",
      "Step: 74, Loss: 0.19761300086975098\n",
      "Step: 75, Loss: 0.19771365821361542\n",
      "Step: 76, Loss: 0.19764795899391174\n",
      "Step: 77, Loss: 0.19760073721408844\n",
      "Step: 78, Loss: 0.19768673181533813\n",
      "Step: 79, Loss: 0.19767118990421295\n",
      "Step: 80, Loss: 0.19763650000095367\n",
      "Step: 81, Loss: 0.19754160940647125\n",
      "Step: 82, Loss: 0.1976841539144516\n",
      "Step: 83, Loss: 0.19770650565624237\n",
      "Step: 84, Loss: 0.19757075607776642\n",
      "Step: 85, Loss: 0.19753405451774597\n",
      "Step: 86, Loss: 0.19765415787696838\n",
      "Step: 87, Loss: 0.197690948843956\n",
      "Step: 88, Loss: 0.19760677218437195\n",
      "Step: 89, Loss: 0.1978646069765091\n",
      "Step: 90, Loss: 0.19760845601558685\n",
      "Step: 91, Loss: 0.19760264456272125\n",
      "Step: 92, Loss: 0.19762660562992096\n",
      "Step: 93, Loss: 0.1975744068622589\n",
      "Step: 94, Loss: 0.19766418635845184\n",
      "Step: 95, Loss: 0.19763118028640747\n",
      "Step: 96, Loss: 0.19764408469200134\n",
      "Step: 97, Loss: 0.19766037166118622\n",
      "Step: 98, Loss: 0.197661891579628\n",
      "Step: 99, Loss: 0.19764713943004608\n",
      "0.14297160506248474\n",
      "Predicting for y\n",
      "Step: 0, Loss: 3.8299617767333984\n",
      "Step: 1, Loss: 3.640282392501831\n",
      "Step: 2, Loss: 3.4522337913513184\n",
      "Step: 3, Loss: 3.2704482078552246\n",
      "Step: 4, Loss: 3.0695981979370117\n",
      "Step: 5, Loss: 2.819753885269165\n",
      "Step: 6, Loss: 2.525777816772461\n",
      "Step: 7, Loss: 2.2060279846191406\n",
      "Step: 8, Loss: 1.813828706741333\n",
      "Step: 9, Loss: 1.3932489156723022\n",
      "Step: 10, Loss: 0.9339903593063354\n",
      "Step: 11, Loss: 0.48479756712913513\n",
      "Step: 12, Loss: 0.16044582426548004\n",
      "Step: 13, Loss: 0.1025974228978157\n",
      "Step: 14, Loss: 0.344256728887558\n",
      "Step: 15, Loss: 0.47265857458114624\n",
      "Step: 16, Loss: 0.418672114610672\n",
      "Step: 17, Loss: 0.28545230627059937\n",
      "Step: 18, Loss: 0.1576739102602005\n",
      "Step: 19, Loss: 0.07933399081230164\n",
      "Step: 20, Loss: 0.04397958517074585\n",
      "Step: 21, Loss: 0.042439818382263184\n",
      "Step: 22, Loss: 0.060026802122592926\n",
      "Step: 23, Loss: 0.0809205025434494\n",
      "Step: 24, Loss: 0.09915599972009659\n",
      "Step: 25, Loss: 0.10942040383815765\n",
      "Step: 26, Loss: 0.11129879951477051\n",
      "Step: 27, Loss: 0.1063632220029831\n",
      "Step: 28, Loss: 0.09452646225690842\n",
      "Step: 29, Loss: 0.0798112004995346\n",
      "Step: 30, Loss: 0.06422672420740128\n",
      "Step: 31, Loss: 0.049360133707523346\n",
      "Step: 32, Loss: 0.03595094382762909\n",
      "Step: 33, Loss: 0.02972867526113987\n",
      "Step: 34, Loss: 0.027012676000595093\n",
      "Step: 35, Loss: 0.029281776398420334\n",
      "Step: 36, Loss: 0.03284985572099686\n",
      "Step: 37, Loss: 0.03808567300438881\n",
      "Step: 38, Loss: 0.041851624846458435\n",
      "Step: 39, Loss: 0.042678069323301315\n",
      "Step: 40, Loss: 0.04217454791069031\n",
      "Step: 41, Loss: 0.03841354325413704\n",
      "Step: 42, Loss: 0.033279094845056534\n",
      "Step: 43, Loss: 0.028738398104906082\n",
      "Step: 44, Loss: 0.02377260848879814\n",
      "Step: 45, Loss: 0.021811753511428833\n",
      "Step: 46, Loss: 0.020987099036574364\n",
      "Step: 47, Loss: 0.021087054163217545\n",
      "Step: 48, Loss: 0.02198229730129242\n",
      "Step: 49, Loss: 0.023322394117712975\n",
      "Step: 50, Loss: 0.024345388635993004\n",
      "Step: 51, Loss: 0.0252483319491148\n",
      "Step: 52, Loss: 0.024826552718877792\n",
      "Step: 53, Loss: 0.024097425863146782\n",
      "Step: 54, Loss: 0.02308141626417637\n",
      "Step: 55, Loss: 0.021206039935350418\n",
      "Step: 56, Loss: 0.019518813118338585\n",
      "Step: 57, Loss: 0.018585262820124626\n",
      "Step: 58, Loss: 0.018021635711193085\n",
      "Step: 59, Loss: 0.01787080243229866\n",
      "Step: 60, Loss: 0.017855346202850342\n",
      "Step: 61, Loss: 0.01869463175535202\n",
      "Step: 62, Loss: 0.01845220848917961\n",
      "Step: 63, Loss: 0.01889464072883129\n",
      "Step: 64, Loss: 0.018725408241152763\n",
      "Step: 65, Loss: 0.01860189624130726\n",
      "Step: 66, Loss: 0.018448108807206154\n",
      "Step: 67, Loss: 0.01776924915611744\n",
      "Step: 68, Loss: 0.01739524118602276\n",
      "Step: 69, Loss: 0.01692090556025505\n",
      "Step: 70, Loss: 0.016810564324259758\n",
      "Step: 71, Loss: 0.016029024496674538\n",
      "Step: 72, Loss: 0.016367774456739426\n",
      "Step: 73, Loss: 0.016827436164021492\n",
      "Step: 74, Loss: 0.016631243750452995\n",
      "Step: 75, Loss: 0.01654379442334175\n",
      "Step: 76, Loss: 0.01632002554833889\n",
      "Step: 77, Loss: 0.01602303422987461\n",
      "Step: 78, Loss: 0.016204342246055603\n",
      "Step: 79, Loss: 0.015888208523392677\n",
      "Step: 80, Loss: 0.0161662008613348\n",
      "Step: 81, Loss: 0.01570364087820053\n",
      "Step: 82, Loss: 0.015544412657618523\n",
      "Step: 83, Loss: 0.015655722469091415\n",
      "Step: 84, Loss: 0.015495452098548412\n",
      "Step: 85, Loss: 0.015467938035726547\n",
      "Step: 86, Loss: 0.015473741106688976\n",
      "Step: 87, Loss: 0.015445569530129433\n",
      "Step: 88, Loss: 0.015183213166892529\n",
      "Step: 89, Loss: 0.01563461683690548\n",
      "Step: 90, Loss: 0.015184679999947548\n",
      "Step: 91, Loss: 0.015358754433691502\n",
      "Step: 92, Loss: 0.014824391342699528\n",
      "Step: 93, Loss: 0.014916016720235348\n",
      "Step: 94, Loss: 0.015146206133067608\n",
      "Step: 95, Loss: 0.014843684621155262\n",
      "Step: 96, Loss: 0.015041208826005459\n",
      "Step: 97, Loss: 0.014979224652051926\n",
      "Step: 98, Loss: 0.014833787456154823\n",
      "Step: 99, Loss: 0.01504676416516304\n",
      "0.007528424728661776\n",
      "Predicting for z\n",
      "Step: 0, Loss: 0.4901421368122101\n",
      "Step: 1, Loss: 0.4321138262748718\n",
      "Step: 2, Loss: 0.3833988606929779\n",
      "Step: 3, Loss: 0.33671319484710693\n",
      "Step: 4, Loss: 0.294023722410202\n",
      "Step: 5, Loss: 0.24799104034900665\n",
      "Step: 6, Loss: 0.2136593610048294\n",
      "Step: 7, Loss: 0.18781237304210663\n",
      "Step: 8, Loss: 0.18114250898361206\n",
      "Step: 9, Loss: 0.19291576743125916\n",
      "Step: 10, Loss: 0.198977530002594\n",
      "Step: 11, Loss: 0.19293484091758728\n",
      "Step: 12, Loss: 0.18215662240982056\n",
      "Step: 13, Loss: 0.17351923882961273\n",
      "Step: 14, Loss: 0.1677047461271286\n",
      "Step: 15, Loss: 0.16467472910881042\n",
      "Step: 16, Loss: 0.1635531485080719\n",
      "Step: 17, Loss: 0.16382113099098206\n",
      "Step: 18, Loss: 0.16611482203006744\n",
      "Step: 19, Loss: 0.16786077618598938\n",
      "Step: 20, Loss: 0.16892899572849274\n",
      "Step: 21, Loss: 0.16729116439819336\n",
      "Step: 22, Loss: 0.16382534801959991\n",
      "Step: 23, Loss: 0.1634671837091446\n",
      "Step: 24, Loss: 0.16077746450901031\n",
      "Step: 25, Loss: 0.15861955285072327\n",
      "Step: 26, Loss: 0.15772061049938202\n",
      "Step: 27, Loss: 0.15718375146389008\n",
      "Step: 28, Loss: 0.15625573694705963\n",
      "Step: 29, Loss: 0.15698257088661194\n",
      "Step: 30, Loss: 0.15688423812389374\n",
      "Step: 31, Loss: 0.1565617173910141\n",
      "Step: 32, Loss: 0.15797655284404755\n",
      "Step: 33, Loss: 0.15589724481105804\n",
      "Step: 34, Loss: 0.15561267733573914\n",
      "Step: 35, Loss: 0.15375886857509613\n",
      "Step: 36, Loss: 0.1536092907190323\n",
      "Step: 37, Loss: 0.1539592742919922\n",
      "Step: 38, Loss: 0.15413250029087067\n",
      "Step: 39, Loss: 0.15425916016101837\n",
      "Step: 40, Loss: 0.1540193408727646\n",
      "Step: 41, Loss: 0.15451399981975555\n",
      "Step: 42, Loss: 0.15420672297477722\n",
      "Step: 43, Loss: 0.15358784794807434\n",
      "Step: 44, Loss: 0.1538742482662201\n",
      "Step: 45, Loss: 0.15369758009910583\n",
      "Step: 46, Loss: 0.15262535214424133\n",
      "Step: 47, Loss: 0.15255782008171082\n",
      "Step: 48, Loss: 0.15302719175815582\n",
      "Step: 49, Loss: 0.152541846036911\n",
      "Step: 50, Loss: 0.15254995226860046\n",
      "Step: 51, Loss: 0.15276774764060974\n",
      "Step: 52, Loss: 0.15276649594306946\n",
      "Step: 53, Loss: 0.15287663042545319\n",
      "Step: 54, Loss: 0.15241917967796326\n",
      "Step: 55, Loss: 0.15233048796653748\n",
      "Step: 56, Loss: 0.1523294597864151\n",
      "Step: 57, Loss: 0.15190406143665314\n",
      "Step: 58, Loss: 0.15223552286624908\n",
      "Step: 59, Loss: 0.15187415480613708\n",
      "Step: 60, Loss: 0.15224497020244598\n",
      "Step: 61, Loss: 0.1524926722049713\n",
      "Step: 62, Loss: 0.15263941884040833\n",
      "Step: 63, Loss: 0.1517811417579651\n",
      "Step: 64, Loss: 0.1517622172832489\n",
      "Step: 65, Loss: 0.15118668973445892\n",
      "Step: 66, Loss: 0.1519087702035904\n",
      "Step: 67, Loss: 0.15222474932670593\n",
      "Step: 68, Loss: 0.15167950093746185\n",
      "Step: 69, Loss: 0.15196791291236877\n",
      "Step: 70, Loss: 0.15213224291801453\n",
      "Step: 71, Loss: 0.15211565792560577\n",
      "Step: 72, Loss: 0.15199033915996552\n",
      "Step: 73, Loss: 0.1518651843070984\n",
      "Step: 74, Loss: 0.15194404125213623\n",
      "Step: 75, Loss: 0.15169546008110046\n",
      "Step: 76, Loss: 0.1514352262020111\n",
      "Step: 77, Loss: 0.15192651748657227\n",
      "Step: 78, Loss: 0.15153057873249054\n",
      "Step: 79, Loss: 0.15190555155277252\n",
      "Step: 80, Loss: 0.15142560005187988\n",
      "Step: 81, Loss: 0.15150952339172363\n",
      "Step: 82, Loss: 0.15119078755378723\n",
      "Step: 83, Loss: 0.15169383585453033\n",
      "Step: 84, Loss: 0.15147709846496582\n",
      "Step: 85, Loss: 0.15134304761886597\n",
      "Step: 86, Loss: 0.1512879878282547\n",
      "Step: 87, Loss: 0.15142343938350677\n",
      "Step: 88, Loss: 0.15142479538917542\n",
      "Step: 89, Loss: 0.15095342695713043\n",
      "Step: 90, Loss: 0.15154200792312622\n",
      "Step: 91, Loss: 0.15151722729206085\n",
      "Step: 92, Loss: 0.15192216634750366\n",
      "Step: 93, Loss: 0.15139274299144745\n",
      "Step: 94, Loss: 0.15128156542778015\n",
      "Step: 95, Loss: 0.15159034729003906\n",
      "Step: 96, Loss: 0.15169942378997803\n",
      "Step: 97, Loss: 0.15137188136577606\n",
      "Step: 98, Loss: 0.15107324719429016\n",
      "Step: 99, Loss: 0.15126988291740417\n",
      "0.06939423829317093\n",
      "Predicting for HMDRX\n",
      "Step: 0, Loss: 7042.71875\n",
      "Step: 1, Loss: 7039.8916015625\n",
      "Step: 2, Loss: 7037.095703125\n",
      "Step: 3, Loss: 7034.14208984375\n",
      "Step: 4, Loss: 7030.2841796875\n",
      "Step: 5, Loss: 7026.048828125\n",
      "Step: 6, Loss: 7020.7470703125\n",
      "Step: 7, Loss: 7014.58154296875\n",
      "Step: 8, Loss: 7006.85302734375\n",
      "Step: 9, Loss: 6996.6484375\n",
      "Step: 10, Loss: 6985.2392578125\n",
      "Step: 11, Loss: 6971.6171875\n",
      "Step: 12, Loss: 6952.703125\n",
      "Step: 13, Loss: 6929.66015625\n",
      "Step: 14, Loss: 6904.02294921875\n",
      "Step: 15, Loss: 6871.70068359375\n",
      "Step: 16, Loss: 6839.5849609375\n",
      "Step: 17, Loss: 6805.10693359375\n",
      "Step: 18, Loss: 6773.47509765625\n",
      "Step: 19, Loss: 6744.2421875\n",
      "Step: 20, Loss: 6717.87841796875\n",
      "Step: 21, Loss: 6693.7431640625\n",
      "Step: 22, Loss: 6671.2373046875\n",
      "Step: 23, Loss: 6650.32958984375\n",
      "Step: 24, Loss: 6630.26806640625\n",
      "Step: 25, Loss: 6611.15185546875\n",
      "Step: 26, Loss: 6592.76513671875\n",
      "Step: 27, Loss: 6575.07177734375\n",
      "Step: 28, Loss: 6557.9951171875\n",
      "Step: 29, Loss: 6541.29833984375\n",
      "Step: 30, Loss: 6525.1044921875\n",
      "Step: 31, Loss: 6509.12646484375\n",
      "Step: 32, Loss: 6493.62841796875\n",
      "Step: 33, Loss: 6478.39697265625\n",
      "Step: 34, Loss: 6463.4931640625\n",
      "Step: 35, Loss: 6448.86669921875\n",
      "Step: 36, Loss: 6434.55029296875\n",
      "Step: 37, Loss: 6420.4912109375\n",
      "Step: 38, Loss: 6406.74169921875\n",
      "Step: 39, Loss: 6393.25244140625\n",
      "Step: 40, Loss: 6380.0625\n",
      "Step: 41, Loss: 6367.11279296875\n",
      "Step: 42, Loss: 6354.46533203125\n",
      "Step: 43, Loss: 6342.07568359375\n",
      "Step: 44, Loss: 6329.9560546875\n",
      "Step: 45, Loss: 6318.111328125\n",
      "Step: 46, Loss: 6306.50830078125\n",
      "Step: 47, Loss: 6295.1708984375\n",
      "Step: 48, Loss: 6284.09521484375\n",
      "Step: 49, Loss: 6273.263671875\n",
      "Step: 50, Loss: 6262.666015625\n",
      "Step: 51, Loss: 6252.3564453125\n",
      "Step: 52, Loss: 6242.2421875\n",
      "Step: 53, Loss: 6232.3798828125\n",
      "Step: 54, Loss: 6222.74853515625\n",
      "Step: 55, Loss: 6213.35546875\n",
      "Step: 56, Loss: 6204.181640625\n",
      "Step: 57, Loss: 6195.212890625\n",
      "Step: 58, Loss: 6186.48583984375\n",
      "Step: 59, Loss: 6177.947265625\n",
      "Step: 60, Loss: 6169.63232421875\n",
      "Step: 61, Loss: 6161.509765625\n",
      "Step: 62, Loss: 6153.58984375\n",
      "Step: 63, Loss: 6145.86376953125\n",
      "Step: 64, Loss: 6138.33349609375\n",
      "Step: 65, Loss: 6130.9970703125\n",
      "Step: 66, Loss: 6123.83154296875\n",
      "Step: 67, Loss: 6116.8515625\n",
      "Step: 68, Loss: 6110.0595703125\n",
      "Step: 69, Loss: 6103.41650390625\n",
      "Step: 70, Loss: 6096.96533203125\n",
      "Step: 71, Loss: 6090.6689453125\n",
      "Step: 72, Loss: 6084.533203125\n",
      "Step: 73, Loss: 6078.5595703125\n",
      "Step: 74, Loss: 6072.7421875\n",
      "Step: 75, Loss: 6067.07080078125\n",
      "Step: 76, Loss: 6061.5478515625\n",
      "Step: 77, Loss: 6056.18701171875\n",
      "Step: 78, Loss: 6050.93505859375\n",
      "Step: 79, Loss: 6045.86474609375\n",
      "Step: 80, Loss: 6040.8955078125\n",
      "Step: 81, Loss: 6036.08056640625\n",
      "Step: 82, Loss: 6031.39013671875\n",
      "Step: 83, Loss: 6026.814453125\n",
      "Step: 84, Loss: 6022.36669921875\n",
      "Step: 85, Loss: 6018.05126953125\n",
      "Step: 86, Loss: 6013.84130859375\n",
      "Step: 87, Loss: 6009.7744140625\n",
      "Step: 88, Loss: 6005.79248046875\n",
      "Step: 89, Loss: 6001.92626953125\n",
      "Step: 90, Loss: 5998.17919921875\n",
      "Step: 91, Loss: 5994.52685546875\n",
      "Step: 92, Loss: 5990.9931640625\n",
      "Step: 93, Loss: 5987.54150390625\n",
      "Step: 94, Loss: 5984.19384765625\n",
      "Step: 95, Loss: 5980.95751953125\n",
      "Step: 96, Loss: 5977.806640625\n",
      "Step: 97, Loss: 5974.72412109375\n",
      "Step: 98, Loss: 5971.76416015625\n",
      "Step: 99, Loss: 5968.86572265625\n",
      "261.54742431640625\n",
      "Predicting for HMDRY\n",
      "Step: 0, Loss: 52656.71484375\n",
      "Step: 1, Loss: 52635.26953125\n",
      "Step: 2, Loss: 52617.03515625\n",
      "Step: 3, Loss: 52598.73046875\n",
      "Step: 4, Loss: 52578.6171875\n",
      "Step: 5, Loss: 52555.1484375\n",
      "Step: 6, Loss: 52527.81640625\n",
      "Step: 7, Loss: 52492.58984375\n",
      "Step: 8, Loss: 52449.81640625\n",
      "Step: 9, Loss: 52398.57421875\n",
      "Step: 10, Loss: 52329.39453125\n",
      "Step: 11, Loss: 52245.3828125\n",
      "Step: 12, Loss: 52137.46875\n",
      "Step: 13, Loss: 52000.484375\n",
      "Step: 14, Loss: 51838.01953125\n",
      "Step: 15, Loss: 51659.953125\n",
      "Step: 16, Loss: 51470.37109375\n",
      "Step: 17, Loss: 51289.703125\n",
      "Step: 18, Loss: 51122.265625\n",
      "Step: 19, Loss: 50964.078125\n",
      "Step: 20, Loss: 50820.1328125\n",
      "Step: 21, Loss: 50685.171875\n",
      "Step: 22, Loss: 50559.1796875\n",
      "Step: 23, Loss: 50440.640625\n",
      "Step: 24, Loss: 50326.5703125\n",
      "Step: 25, Loss: 50216.62890625\n",
      "Step: 26, Loss: 50110.59375\n",
      "Step: 27, Loss: 50006.14453125\n",
      "Step: 28, Loss: 49904.359375\n",
      "Step: 29, Loss: 49803.6875\n",
      "Step: 30, Loss: 49704.27734375\n",
      "Step: 31, Loss: 49605.921875\n",
      "Step: 32, Loss: 49508.37109375\n",
      "Step: 33, Loss: 49411.3359375\n",
      "Step: 34, Loss: 49315.12109375\n",
      "Step: 35, Loss: 49219.3359375\n",
      "Step: 36, Loss: 49124.21484375\n",
      "Step: 37, Loss: 49029.5625\n",
      "Step: 38, Loss: 48935.41796875\n",
      "Step: 39, Loss: 48841.83203125\n",
      "Step: 40, Loss: 48748.7265625\n",
      "Step: 41, Loss: 48656.1640625\n",
      "Step: 42, Loss: 48564.15234375\n",
      "Step: 43, Loss: 48472.6171875\n",
      "Step: 44, Loss: 48381.66796875\n",
      "Step: 45, Loss: 48291.17578125\n",
      "Step: 46, Loss: 48201.2265625\n",
      "Step: 47, Loss: 48111.76953125\n",
      "Step: 48, Loss: 48022.8515625\n",
      "Step: 49, Loss: 47934.40234375\n",
      "Step: 50, Loss: 47846.53125\n",
      "Step: 51, Loss: 47759.0625\n",
      "Step: 52, Loss: 47672.1640625\n",
      "Step: 53, Loss: 47585.78125\n",
      "Step: 54, Loss: 47499.87890625\n",
      "Step: 55, Loss: 47414.41796875\n",
      "Step: 56, Loss: 47329.45703125\n",
      "Step: 57, Loss: 47244.953125\n",
      "Step: 58, Loss: 47160.95703125\n",
      "Step: 59, Loss: 47077.36328125\n",
      "Step: 60, Loss: 46994.23046875\n",
      "Step: 61, Loss: 46911.578125\n",
      "Step: 62, Loss: 46829.33203125\n",
      "Step: 63, Loss: 46747.546875\n",
      "Step: 64, Loss: 46666.140625\n",
      "Step: 65, Loss: 46585.203125\n",
      "Step: 66, Loss: 46504.67578125\n",
      "Step: 67, Loss: 46424.57421875\n",
      "Step: 68, Loss: 46344.8359375\n",
      "Step: 69, Loss: 46265.53125\n",
      "Step: 70, Loss: 46186.54296875\n",
      "Step: 71, Loss: 46108.01171875\n",
      "Step: 72, Loss: 46029.83203125\n",
      "Step: 73, Loss: 45952.0546875\n",
      "Step: 74, Loss: 45874.640625\n",
      "Step: 75, Loss: 45797.5390625\n",
      "Step: 76, Loss: 45720.8203125\n",
      "Step: 77, Loss: 45644.4609375\n",
      "Step: 78, Loss: 45568.5\n",
      "Step: 79, Loss: 45492.859375\n",
      "Step: 80, Loss: 45417.5078125\n",
      "Step: 81, Loss: 45342.5390625\n",
      "Step: 82, Loss: 45267.87890625\n",
      "Step: 83, Loss: 45193.55859375\n",
      "Step: 84, Loss: 45119.55859375\n",
      "Step: 85, Loss: 45045.88671875\n",
      "Step: 86, Loss: 44972.51171875\n",
      "Step: 87, Loss: 44899.4453125\n",
      "Step: 88, Loss: 44826.69921875\n",
      "Step: 89, Loss: 44754.27734375\n",
      "Step: 90, Loss: 44682.16796875\n",
      "Step: 91, Loss: 44610.31640625\n",
      "Step: 92, Loss: 44538.734375\n",
      "Step: 93, Loss: 44467.48828125\n",
      "Step: 94, Loss: 44396.51171875\n",
      "Step: 95, Loss: 44325.875\n",
      "Step: 96, Loss: 44255.453125\n",
      "Step: 97, Loss: 44185.33984375\n",
      "Step: 98, Loss: 44115.48828125\n",
      "Step: 99, Loss: 44045.95703125\n",
      "333.64190673828125\n",
      "Predicting for HMDRZ\n",
      "Step: 0, Loss: 86242.7421875\n",
      "Step: 1, Loss: 86216.5390625\n",
      "Step: 2, Loss: 86188.640625\n",
      "Step: 3, Loss: 86158.6171875\n",
      "Step: 4, Loss: 86124.3984375\n",
      "Step: 5, Loss: 86082.9921875\n",
      "Step: 6, Loss: 86033.421875\n",
      "Step: 7, Loss: 85972.3515625\n",
      "Step: 8, Loss: 85897.328125\n",
      "Step: 9, Loss: 85800.703125\n",
      "Step: 10, Loss: 85677.1796875\n",
      "Step: 11, Loss: 85526.2421875\n",
      "Step: 12, Loss: 85332.015625\n",
      "Step: 13, Loss: 85093.5234375\n",
      "Step: 14, Loss: 84824.7578125\n",
      "Step: 15, Loss: 84550.34375\n",
      "Step: 16, Loss: 84281.0859375\n",
      "Step: 17, Loss: 84029.34375\n",
      "Step: 18, Loss: 83797.234375\n",
      "Step: 19, Loss: 83584.0859375\n",
      "Step: 20, Loss: 83383.015625\n",
      "Step: 21, Loss: 83195.8046875\n",
      "Step: 22, Loss: 83018.921875\n",
      "Step: 23, Loss: 82848.78125\n",
      "Step: 24, Loss: 82684.28125\n",
      "Step: 25, Loss: 82525.203125\n",
      "Step: 26, Loss: 82369.4296875\n",
      "Step: 27, Loss: 82215.6875\n",
      "Step: 28, Loss: 82064.1484375\n",
      "Step: 29, Loss: 81914.21875\n",
      "Step: 30, Loss: 81765.765625\n",
      "Step: 31, Loss: 81617.84375\n",
      "Step: 32, Loss: 81471.0\n",
      "Step: 33, Loss: 81324.8125\n",
      "Step: 34, Loss: 81179.3359375\n",
      "Step: 35, Loss: 81034.390625\n",
      "Step: 36, Loss: 80890.234375\n",
      "Step: 37, Loss: 80746.578125\n",
      "Step: 38, Loss: 80603.703125\n",
      "Step: 39, Loss: 80461.328125\n",
      "Step: 40, Loss: 80319.59375\n",
      "Step: 41, Loss: 80178.515625\n",
      "Step: 42, Loss: 80038.0078125\n",
      "Step: 43, Loss: 79898.1796875\n",
      "Step: 44, Loss: 79758.9921875\n",
      "Step: 45, Loss: 79620.3984375\n",
      "Step: 46, Loss: 79482.4375\n",
      "Step: 47, Loss: 79345.21875\n",
      "Step: 48, Loss: 79208.5\n",
      "Step: 49, Loss: 79072.453125\n",
      "Step: 50, Loss: 78937.015625\n",
      "Step: 51, Loss: 78802.203125\n",
      "Step: 52, Loss: 78667.9921875\n",
      "Step: 53, Loss: 78534.375\n",
      "Step: 54, Loss: 78401.390625\n",
      "Step: 55, Loss: 78268.9453125\n",
      "Step: 56, Loss: 78137.078125\n",
      "Step: 57, Loss: 78005.84375\n",
      "Step: 58, Loss: 77875.1171875\n",
      "Step: 59, Loss: 77744.96875\n",
      "Step: 60, Loss: 77615.328125\n",
      "Step: 61, Loss: 77486.2578125\n",
      "Step: 62, Loss: 77357.7421875\n",
      "Step: 63, Loss: 77229.7421875\n",
      "Step: 64, Loss: 77102.2265625\n",
      "Step: 65, Loss: 76975.1640625\n",
      "Step: 66, Loss: 76848.7421875\n",
      "Step: 67, Loss: 76722.734375\n",
      "Step: 68, Loss: 76597.1640625\n",
      "Step: 69, Loss: 76472.140625\n",
      "Step: 70, Loss: 76347.5390625\n",
      "Step: 71, Loss: 76223.4296875\n",
      "Step: 72, Loss: 76099.7578125\n",
      "Step: 73, Loss: 75976.515625\n",
      "Step: 74, Loss: 75853.6796875\n",
      "Step: 75, Loss: 75731.3515625\n",
      "Step: 76, Loss: 75609.421875\n",
      "Step: 77, Loss: 75487.90625\n",
      "Step: 78, Loss: 75366.7890625\n",
      "Step: 79, Loss: 75246.0859375\n",
      "Step: 80, Loss: 75125.8046875\n",
      "Step: 81, Loss: 75005.921875\n",
      "Step: 82, Loss: 74886.375\n",
      "Step: 83, Loss: 74767.3203125\n",
      "Step: 84, Loss: 74648.546875\n",
      "Step: 85, Loss: 74530.1484375\n",
      "Step: 86, Loss: 74412.1953125\n",
      "Step: 87, Loss: 74294.515625\n",
      "Step: 88, Loss: 74177.296875\n",
      "Step: 89, Loss: 74060.3828125\n",
      "Step: 90, Loss: 73943.8046875\n",
      "Step: 91, Loss: 73827.625\n",
      "Step: 92, Loss: 73711.7109375\n",
      "Step: 93, Loss: 73596.21875\n",
      "Step: 94, Loss: 73480.9765625\n",
      "Step: 95, Loss: 73366.09375\n",
      "Step: 96, Loss: 73251.5546875\n",
      "Step: 97, Loss: 73137.375\n",
      "Step: 98, Loss: 73023.4296875\n",
      "Step: 99, Loss: 72909.890625\n",
      "106396.3125\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    fileName = r\"dataset/H\" + str(i) +  \".csv\"\n",
    "    resultsFolder = r\"dataset/H\" + str(i) + \"_nav/\"\n",
    "    plotsFolderName = r\"dataset/H\" + str(i) + \"_nav/plots/\"\n",
    "    df = pd.read_csv (fileName)\n",
    "    \n",
    "    for (columnName, columnData) in df.iteritems():\n",
    "        print(\"Predicting for \" + columnName)\n",
    "        dataCol = df[columnName].to_numpy()\n",
    "        trainDataX, trainDataY, testDataX, testDataY = generateDataset(dataCol)\n",
    "        \n",
    "        model = LSTM2(numLayers = 10)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimiser = optim.Adam(model.parameters(), lr=0.01)\n",
    "        trainModel(100, model, optimiser, criterion, trainDataX, trainDataY,plotsFolderName,columnName)\n",
    "        torch.save(model.state_dict(), resultsFolder+columnName+\"savedModel.pt\")\n",
    "        evaluateModel(model, criterion, testDataX, testDataY, plotsFolderName, columnName)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71ddd5e9409eaf14d4d4bd6327be54b02490eb403d295c16528195f3b190b56b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('multimodal-priors')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
